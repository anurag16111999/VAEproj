{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"#!/usr/bin/env python\n# coding: utf-8\n\n# In[1]:\n\n\n#!/usr/bin/env python\n\n\n# In[1]:\n\n\n#!/usr/bin/env python\n\n\n# In[1]:\n\n\n'''Example of VAE on MNIST dataset using MLP\n\nThe VAE has a modular design. The encoder, decoder and VAE\nare 3 models that share weights. After training the VAE model,\nthe encoder can be used to generate latent vectors.\nThe decoder can be used to generate MNIST digits by sampling the\nlatent vector from a Gaussian distribution with mean = 0 and std = 1.\n\n# Reference\n\n[1] Kingma, Diederik P., and Max Welling.\n\"Auto-Encoding Variational Bayes.\"\nhttps://arxiv.org/abs/1312.6114\n'''\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom csv import reader\n\n# import\n\nfrom keras.layers import Lambda, Input, Dense\nfrom keras.layers import Dropout\n# keras lambda layer\nfrom keras.models import Model\n# from keras.datasets import mnist\nfrom keras.losses import mse, binary_crossentropy,categorical_crossentropy\nfrom keras.utils import plot_model\nfrom keras import backend as K\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport argparse\nimport os\n\nimport tensorflow as tf\n# from keras.layers import Input, Dense\n# from keras.models import Model\nimport csv\nfrom os import listdir\nfrom os.path import isfile, join\nfrom scipy.misc import imread\n# import numpy as np\nimport cv2\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom numpy import linalg as LA\nimport matplotlib.pyplot as plt\nimport random as rd\n# import cv2\n\n# reparameterization trick\n# instead of sampling from Q(z|X), sample epsilon = N(0,I)\n# z = z_mean + sqrt(var) * epsilon\n\n\n# In[2]:\n\n# inputno = 0/0\n# inp11 = 0\n# out11 = 0\n\n# def getnewloss(x2):\n\n#     outputno = outputno + 1\n#     return binary_crossentropy(x1,x2)\n\n\n# def getloss(x1):\n#     # output = outputs[inputno]\n#     # inputindex = imindex[inputno]\n#     outputno = 0\n#     losses1 = tf.map_fn(lambda x : getnewloss(x),outputs)\n#     inputno = inputno + 1\n#     return tf.reduce_sum(losses1)\n\n\n\ndef classcrossentropy(inputs,outputs):\n    loss = 0\n    i = 0\n\n    input1 = inputs[1]\n    output1 = outputs[0]\n    recon_loss = -tf.reduce_sum(\n    input1 * tf.log(1e-5+output1) + \n    (1-input1) * tf.log(1e-5+1-output1), \n    axis=1\n)\n    input2 = inputs[2]\n    output2 = outputs[1]\n    \n#     print(input2.shape)\n#     print(output2.shape)\n    \n#     print(inputs.shape)\n    recon_loss = recon_loss + classifierwt*categorical_crossentropy(input2,output2)\n    \n    \n    return recon_loss\n\n\n\n    # print(tf.shape(inputs))\n    # print(outputs.shape)\n    # print(inputs[0].shape)\n    # print(inputs[1].shape)\n    \n    # print(inputs.shape)\n    #\n    # return binary_crossentropy(inputs[1],outputs)\n    \n#     tf.map_fn(\n#     fn,\n#     elems,\n#     dtype=None,\n#     parallel_iterations=None,\n#     back_prop=True,\n#     swap_memory=False,\n#     infer_shape=True,\n#     name=None\n# )\n                                   \n    # inp11 = inputs[0]\n    # out11 = outputs                                                     \n    # iminput = inputs[0]\n    # imindex = inputs[1]\n    # indices = [i for i, x in enumerate(my_list) if x == \"whatever\"]\n    # outputs[:] = outputs[:].append(imindex)\n    # for rec in outputs:\n        # loss = rec\n    # outputno = 0\n    # inputno = 0\n    # losses = tf.map_fn(lambda x : getloss(x),)\n\n    # return tf.reduce_sum(losses,axis = 1)\n\n    # inputs = inputs[0]\n\n    # epsilon = 1e-10\n    # epsilon = epsilon*np.ones((128,2048))\n\n    # recon_loss = -tf.reduce_sum(\n    #     inputs * tf.log(1e-10+outputs) + \n    #     (1-inputs) * tf.log(1e-10+1-outputs), \n    #     axis=1\n    # )\n\n    # recon_loss = tf.reduce_mean(recon_loss)\n\n    # Latent loss\n    # KL divergence: measure the difference between two distributions\n    # Here we measure the divergence between \n    # the latent distribution and N(0, 1)\n    # latent_loss = -0.5 * tf.reduce_sum(\n    #     1 + z_log_sigma_sq - tf.square(z_mu) - \n    #     tf.exp(z_log_sigma_sq), axis=1)\n    # latent_loss = tf.reduce_mean(latent_loss)\n\n    # total_loss = recon_loss + latent_loss\n    # return recon_loss\n\n    # j = 0\n    # # return binary_crossentropy(inputs[0],outputs)\n    # inputim = inputs[0]\n    # inputlabel = inputs[1]\n\n    # for i in range(batch_size):\n    #     # j = 0\n    #     for j in range(batch_size):\n    #         # j+= 1\n    #         # input1 = inputim[i]\n    #         input2 = inputim[j]\n    #         k1 = inputlabel[i]\n    #         k2 = inputlabel[j]\n    #         output1 = outputs[i]\n    #         # print(k1)\n    #         # print(k2)\n    #         # if(k1 == k2):\n    #         loss = loss + binary_crossentropy(input2,output1)\n    #     # i+= 1\n    # return loss\n\n\n\n\n    # print(inputs1.shape)\n    # # if(runmode == \"single\"):\n    # # return binary_crossentropy(inputs,outputs)\n    \n\n    # i = 0\n    # for inputs in inputs1:\n    #     outputs = outputs1[i]\n    #     imlabel = 0\n    #     i1 = 0\n    #     for im1 in flowerlist:\n    #         # if(im1 == inputs):\n    #         if(np.array_equal(im1,inputs)):\n    #             imlabel = labels[i1]\n    #             break\n    #         i1 = i1+1\n    #     print(imlabel)\n    # #     for key in dict1:    \n    #     loss = 0\n    #     i1 = 0\n    #     for image1 in flowerlist:\n    #         if(labels[i1] == imlabel):\n    #             if(loss == 0):\n    #                 loss = binary_crossentropy(image1,outputs)\n    #             else:\n    #                 loss = loss + binary_crossentropy(image1,outputs)\n    #         i1 += 1\n    #     # return binary_crossentropy(inputs,outputs)\n    #     print(loss)\n    #     print(binary_crossentropy(image1,outputs))\n    #     i = i+1\n    # return K.sum(loss)\n\n\n# In[2]:\n\n\ndef sampling(args):\n    \"\"\"Reparameterization trick by sampling from an isotropic unit Gaussian.\n\n    # Arguments\n        args (tensor): mean and log of variance of Q(z|X)\n\n    # Returns\n        z (tensor): sampled latent vector\n    \"\"\"\n\n    z_mean, z_log_var = args\n    batch = K.shape(z_mean)[0]\n    dim = K.int_shape(z_mean)[1]\n    # by default, random_normal has mean = 0 and std = 1.0\n    epsilon = K.random_normal(shape=(batch, dim))\n    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n\n\n# In[3]:\n\n\n# In[3]:\n\n\ndef plot_results(models,\n                 data,\n                 batch_size=128,\n                 model_name=\"vae_mnist\"):\n    \"\"\"Plots labels and MNIST digits as a function of the 2D latent vector\n\n    # Arguments\n        models (tuple): encoder and decoder models\n        data (tuple): test data and label\n        batch_size (int): prediction batch size\n        model_name (string): which model is using this function\n    \"\"\"\n\n    encoder, decoder = models\n    x_test, y_test = data\n    os.makedirs(model_name, exist_ok=True)\n\n    filename = os.path.join(model_name, \"vae_mean.png\")\n    # display a 2D plot of the digit classes in the latent space\n    z_mean, _, _ = encoder.predict(x_test,\n                                   batch_size=batch_size)\n    plt.figure(figsize=(12, 10))\n    plt.scatter(z_mean[:, 0], z_mean[:, 1], c=y_test)\n    plt.colorbar()\n    plt.xlabel(\"z[0]\")\n    plt.ylabel(\"z[1]\")\n    plt.savefig(filename)\n    plt.show()\n\n    filename = os.path.join(model_name, \"digits_over_latent.png\")\n    # display a 30x30 2D manifold of digits\n    n = 30\n    digit_size = 28\n    figure = np.zeros((digit_size * n, digit_size * n))\n    # linearly spaced coordinates corresponding to the 2D plot\n    # of digit classes in the latent space\n    grid_x = np.linspace(-4, 4, n)\n    grid_y = np.linspace(-4, 4, n)[::-1]\n\n    for i, yi in enumerate(grid_y):\n        for j, xi in enumerate(grid_x):\n            z_sample = np.array([[xi, yi]])\n            x_decoded = decoder.predict(z_sample)\n            digit = x_decoded[0].reshape(digit_size, digit_size)\n            figure[i * digit_size: (i + 1) * digit_size,\n                   j * digit_size: (j + 1) * digit_size] = digit\n\n    plt.figure(figsize=(10, 10))\n    start_range = digit_size // 2\n    end_range = (n - 1) * digit_size + start_range + 1\n    pixel_range = np.arange(start_range, end_range, digit_size)\n    sample_range_x = np.round(grid_x, 1)\n    sample_range_y = np.round(grid_y, 1)\n    plt.xticks(pixel_range, sample_range_x)\n    plt.yticks(pixel_range, sample_range_y)\n    plt.xlabel(\"z[0]\")\n    plt.ylabel(\"z[1]\")\n    plt.imshow(figure, cmap='Greys_r')\n    plt.savefig(filename)\n    plt.show()\n\n\n# In[6]:","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# In[4]:\n# folder = \"jpg\"\ndef load_csv(filename):\n    dataset = list()\n    with open(filename, 'r') as file:\n        csv_reader = reader(file)\n        for row in csv_reader:\n            if not row:\n                continue\n            dataset.append(row)\n    return dataset\n\n\n# In[9]:\n\n\n# In[17]:\n\n\n# dict1 = {}\n# mypath = \"/home/anurag/projects/VAEProj/102flowers/jpg\"\n# onlyfiles = [f for f in listdir(mypath) if isfile(join(mypath, f))]\n# sort(onlyfiles)\n# onlyfiles.sort()\nfilename = '../input/flowersXception.csv'\n\nfeaturestotal = load_csv(filename)\n# labels = labels[0]    \n# print(onlyfiles)\n# i1 = 1\n# seltol = 1300\n#seltol = 10\n# selected1 = 800\n# for selected1 in [200]:\n# samplpositive = 600\n#samplpositive = 10\n# featurestotal = []\n# i1 = 0\n# for image in onlyfiles:\n#     if(image[-3:] != \"jpg\"):\n#         continue\n#     # i1 = i1 + 1\n#     # print(\"efrffrwf\")\n\n#     im1 = mypath+\"/\"+ image\n#     # print(im1)\n#     # print(labels[i1])\n#     # im = imread(im1,cv2.COLOR_BGR2GRAY)\n#     im = imread(im1)\n#     im = im/float(255)\n#     # print(im)\n#     res = cv2.resize(im, dsize=(50,50), interpolation=cv2.INTER_CUBIC)\n#     #res = cv2.resize(im, dsize=(500,500))\n#     # res = im\n#     # k1\n\n\n#     k1 = np.reshape(res,-1)\n#     # np.append(k1,labels[i1])\n#     # print(k)\n#     k1 = list(k1)\n#     k1.append(labels[i1])\n#     # print(k1)\n\n# #     dict1[k1] = labels[i1] \n#     # print(k1.shape)\n#         # print(i1)\n#     print(i1)\n#     featurestotal.append(k1)\n#     i1 = i1+ 1\n#     # if(i1 == 85):\n#         # break\n\n\n\n# flowerlist = []\n\nrd.shuffle(featurestotal)\n# flowerlist.extend(featurestotal)\n\n# flowerlist contains the required entries\n\n\n\nx_train = featurestotal[:8180]\nx_test = featurestotal[8180:]\n\n\n# x_train = featurestotal[:4000]\n# x_test = featurestotal[4000:]\n\n\n# x_train = featurestotal[:400]\n# x_test = featurestotal[400:]\n\n\nx_train = np.array(x_train,dtype = float)\nx_test = np.array(x_test,dtype = float)\n\nprint(x_train)\nprint(x_test)\n\n\nx_trainlabel = x_train[:,2048]\nx_train = x_train[:,0:2048]\nx_testlabel = x_test[:,2048]\nx_test = x_test[:,0:2048]\n\nprint(x_testlabel)\nprint(x_trainlabel)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"a = np.zeros(2048,dtype = float)\n # = np.repeat(a[:, :, np.newaxis], 3, axis=2)\n\nx_trainlabelsum = np.repeat(a[np.newaxis,:], 102,axis = 0)\nx_testlabelsum = np.repeat(a[np.newaxis,:], 102,axis = 0)\nprint(x_trainlabelsum[0].shape)\n\n# x_testlabelsum = []\n\n\n# In[18]:\n\n\ntrainnum = np.zeros(102)\ntestnum = np.zeros(102)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# In[18]:\ntrainonehotlabel = []\ntestonehotlabel = []\n\n\ni = 0\nfor fq in x_trainlabel:\n    image = x_train[i]\n    fq = int(fq)\n    # print(fq)\n    fq = fq-1\n#     print(fq)\n#     print(x_trainlabelsum[fq].shape)\n#     print(type(image))\n#     print(image.shape)\n#     print(x_trainlabelsum[fq])\n# #     print(type(image))\n#     print(image)\n    \n    x_trainlabelsum[fq] = np.add(x_trainlabelsum[fq],image)\n    trainnum[fq] = trainnum[fq] + 1\n    arr = np.zeros(102)\n    arr[fq] = 1\n    trainonehotlabel.append(arr)\n\n    i = i+1\n\ni = 0\nfor fq in x_testlabel:\n    fq = fq-1\n    im = x_test[i]\n    fq = int(fq)\n    x_testlabelsum[fq] = np.add(x_testlabelsum[fq],im)\n    testnum[fq] = testnum[fq] + 1\n\n    arr = np.zeros(102)\n    arr[fq] = 1\n    testonehotlabel.append(arr)\n\n    i = i+1\n\nprint(x_trainlabel[0:4])\nprint(x_testlabel)\n\n\ntrainonehotlabel = np.array(trainonehotlabel)\ntestonehotlabel = np.array(testonehotlabel)\nprint(trainonehotlabel)\nprint(testonehotlabel)\n\n# exit()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train2 = np.copy(x_train)\nx_test2 = np.copy(x_test)\n\ni = 0\nfor d2 in x_trainlabel:\n    d2 = int(d2)\n    d2 = d2-1\n    x_train2[i] = x_trainlabelsum[d2]/(float(trainnum[d2]))\n    i = i+1\n\ni = 0\nfor d2 in x_testlabel:\n    d2 = int(d2)\n    d2 = d2-1\n    x_test2[i] = x_testlabelsum[d2]/(float(trainnum[d2]))\n    i = i+1\n\n# x_train\n\n# x_trainlabel = np.array(x_trainlabel)\n# x_testlabel = np.array(x_testlabel)\n\n# x_train\n\n\n# print(classcrossentropy(x_train[0],x_train[1]))\n# MNIST dataset\n# (x_train, y_train), (x_test, y_test) = mnist.load_data()\n\n\n# In[3]:\n\n\n# In[19]:","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# image_size = x_train[0].shape[0]\n# print(image_size)\noriginal_dim = 2048\n# x_train = np.reshape(x_train, [-1, original_dim])\n# x_test = np.reshape(x_test, [-1, original_dim])\n# x_train = x_train.astype('float32') / 255\n# x_test = x_test.astype('float32') / 255\n# print(im)\n# network parameters\ninput_shape = (original_dim, )\n# print(\"input_shape\")\n# print(input_shape)\nintermediate_dim = 512 # of original layers\nintermediate_dim1 = 512 # of extra layers\nclassifierdim = 512 # of classifiers\nclassifieroutputdim = 102\n\nbatch_size = 128\nlatent_dim = 2\nepochs = 300\n\n\n# model.add(Dense(60, input_dim=60, kernel_initializer='normal', activation='relu', kernel_constraint=maxnorm(3)))\n# model.add(Dropout(0.2))\n# model.add(Dense(30, kernel_initializer='normal', activation='relu', kernel_constraint=maxnorm(3)))\n# model.add(Dropout(0.2))\n# model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n\n# VAE model = encoder + decoder\n# build encoder model\ninput1 = Input(shape=input_shape, name='encoder_input_im')\ninput2 = Input(shape=input_shape, name='encoder_input_imsum')\ninput3 = Input(shape=(classifieroutputdim,),name='encoder_input_labels')\n# inputs = Input(shape=1, name='encoder_input_label')\n# drop1 = Dropout(0.2)(input1)\nx = Dense(intermediate_dim, activation='relu')(input1)\nx = Dense(intermediate_dim1, activation='relu')(x)\n# x = Dense(intermediate_dim1, activation='relu')(x)\n\n# x = Dense(intermediate_dim1, activation='relu')(x)\n# drop1 = Dropout(0.2)(x)\nz_mean = Dense(latent_dim, name='z_mean')(x)\nz_log_var = Dense(latent_dim, name='z_log_var')(x)\n\ninps = [input1,input2,input3]\n# use reparameterization trick to push the sampling out as input\n# note that \"output_shape\" isn't necessary with the TensorFlow backend\nz = Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])\n\n# instantiate encoder model\n\n# encoder = Model(inputs, [z_mean, z_log_var, z], name='encoder')\n# encoder = Model(inputs = [input1,input2], [z_mean, z_log_var, z], name='encoder')\nencoder = Model(inps, [z_mean, z_log_var, z], name='encoder')\nencoder.summary()\n# plot_model(encoder, to_file='vae_mlp_encoder.png', show_shapes=True)\n\n# build decoder model\nlatent_inputs = Input(shape=(latent_dim,), name='z_samples')\n\nx = Dense(intermediate_dim, activation='relu')(latent_inputs)\n# drop2 = Dropout(0.2)(x)\n# x = Dense(intermediate_dim1, activation='relu')(x)\nx = Dense(intermediate_dim1, activation='relu')(x)\n# x = Dense(intermediate_dim1, activation='relu')(x)\n\noutputs = Dense(original_dim, activation='sigmoid')(x)\n\n\n# instantiate decoder model\ndecoder = Model(latent_inputs, outputs, name='decoder')\ndecoder.summary()\n# plot_model(decoder, to_file='vae_mlp_decoder.png', show_shapes=True)\n\n#building classifier models\nclassifier_inputs = Input(shape=(latent_dim,), name=\"class1\")\n\nx = Dense(classifierdim,activation = 'relu')(classifier_inputs)\n# x = Dense(classifierdim,activation = 'relu')(x)  # additional layer\nclassifier_output = Dense(classifieroutputdim,activation = 'softmax')(x)\n\nclassifier = Model(classifier_inputs,classifier_output,name=\"classifier\")\n\nclassifier.summary()\n\n# instantiate VAE model\noutput1 = decoder(encoder(inps)[2])\noutput2 = classifier(encoder(inps)[2]) # classifing on smaple values\n# output2 = classifier(encoder(inps)[0]) # classifing on z_means\noutputs = [output1,output2]\n\nvae = Model(inps, outputs, name='vae_mlp + classifier')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# inputs = Input(shape=input_shape, name='encoder_input')\n# x = Dense(intermediate_dim, activation='relu')(inputs)\n# z_mean = Dense(latent_dim, name='z_mean')(x)\n# z_log_var = Dense(latent_dim, name='z_log_var')(x)\n\n# # use reparameterization trick to push the sampling out as input\n# # note that \"output_shape\" isn't necessary with the TensorFlow backend\n# z = Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])\n\n# # instantiate encoder model\n# encoder = Model(inputs, [z_mean, z_log_var, z], name='encoder')\n# encoder.summary()\n# # plot_model(encoder, to_file='vae_mlp_encoder.png', show_shapes=True)\n\n# # build decoder model\n# latent_inputs = Input(shape=(latent_dim,), name='z_sampling')\n# x = Dense(intermediate_dim, activation='relu')(latent_inputs)\n# outputs = Dense(original_dim, activation='sigmoid')(x)\n\n# # instantiate decoder model\n# decoder = Model(latent_inputs, outputs, name='decoder')\n# decoder.summary()\n# # plot_model(decoder, to_file='vae_mlp_decoder.png', show_shapes=True)\n\n# # instantiate VAE model\n# outputs = decoder(encoder(inputs)[2])\n# vae = Model(inputs, outputs, name='vae_mlp')\n\nif __name__ == '__main__':\n    # dict1 = {}\n    kllosswt = 1\n    classifierwt = 0.5\n#     parser = argparse.ArgumentParser()\n\n#     help_ = \"Load h5 model trained weights\"\n#     parser.add_argument(\"-w\", \"--weights\", help=help_)\n#     help_ = \"Use mse loss instead of binary cross entropy (default)\"\n#     parser.add_argument(\"-m\",\n#                         \"--mse\",\n#                         help=help_, action='store_true')\n#     args = parser.parse_args()\n    models = (encoder, decoder)\n    # data = (x_test, y_test)\n\n# here1\n    # VAE loss = mse_loss or xent_loss + kl_loss\n    \n    # runmode = input(\"single or class encoder \")\n    # z33 = np.identity(1000)\n\n\n#     if args.mse:\n#         reconstruction_loss = mse(inputs, outputs)\n#     else:\n    reconstruction_loss = classcrossentropy(inps,\n                                              outputs)\n\n    reconstruction_loss *= original_dim\n    kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n    kl_loss = K.sum(kl_loss, axis=-1)\n    kl_loss = kl_loss*-0.5*kllosswt\n    vae_loss = K.mean(reconstruction_loss + kl_loss)\n    vae.add_loss(vae_loss)\n    vae.compile(optimizer='adam')\n    vae.summary()\n    # plot_model(vae,\n               # to_file='vae_mlp.png',\n               # show_shapes=True)\n\n#     if args.weights:\n#         vae.load_weights(args.weights)\n#     else:\n        # train the autoencoder\n    vae.fit([x_train,x_train2,trainonehotlabel],\n            epochs=epochs,\n            batch_size=batch_size,\n            validation_data=([x_test,x_test2,testonehotlabel], None))\n    vae.save_weights('vae_mlp_mnist.h5')\n\n\n\n    # for i in test2\n    # plot_results(models,\n                 # data,\n                 # batch_size=batch_size,\n                 # model_name=\"vae_mlp\")\n\n    encoder, decoder = models\n    # encoder, decoder = models\n    x_test, y_test = [x_train,x_train2,trainonehotlabel],x_trainlabel\n    # os.makedirs(model_name, exist_ok=True)\n\n    # filename = os.path.join(model_name, \"vae_mean.png\")\n    # display a 2D plot of the digit classes in the latent space\n    z_mean, _, _ = encoder.predict(x_test,\n                                   batch_size=batch_size)\n    plt.figure(figsize=(12, 10))\n    plt.scatter(z_mean[:, 0], z_mean[:, 1], c=y_test)\n    plt.colorbar()\n    plt.xlabel(\"z[0]\")\n    plt.ylabel(\"z[1]\")\n    # plt.savefig(filename)\n    plt.show()\n# In[ ]:\n\n\n# In[ ]:","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 2","language":"python","name":"python2"},"language_info":{"codemirror_mode":{"name":"ipython","version":2},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython2","version":"2.7.15rc1"}},"nbformat":4,"nbformat_minor":1}