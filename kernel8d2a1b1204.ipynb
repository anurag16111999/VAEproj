{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "\n",
    "#!/usr/bin/env python\n",
    "\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "\n",
    "#!/usr/bin/env python\n",
    "\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "\n",
    "'''Example of VAE on MNIST dataset using MLP\n",
    "\n",
    "The VAE has a modular design. The encoder, decoder and VAE\n",
    "are 3 models that share weights. After training the VAE model,\n",
    "the encoder can be used to generate latent vectors.\n",
    "The decoder can be used to generate MNIST digits by sampling the\n",
    "latent vector from a Gaussian distribution with mean = 0 and std = 1.\n",
    "\n",
    "# Reference\n",
    "\n",
    "[1] Kingma, Diederik P., and Max Welling.\n",
    "\"Auto-Encoding Variational Bayes.\"\n",
    "https://arxiv.org/abs/1312.6114\n",
    "'''\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from csv import reader\n",
    "\n",
    "# import\n",
    "\n",
    "from keras.layers import Lambda, Input, Dense\n",
    "from keras.layers import Dropout\n",
    "# keras lambda layer\n",
    "from keras.models import Model\n",
    "# from keras.datasets import mnist\n",
    "from keras.losses import mse, binary_crossentropy,categorical_crossentropy\n",
    "from keras.utils import plot_model\n",
    "from keras import backend as K\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "# from keras.layers import Input, Dense\n",
    "# from keras.models import Model\n",
    "import csv\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from scipy.misc import imread\n",
    "# import numpy as np\n",
    "import cv2\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from numpy import linalg as LA\n",
    "import matplotlib.pyplot as plt\n",
    "import random as rd\n",
    "# import cv2\n",
    "\n",
    "# reparameterization trick\n",
    "# instead of sampling from Q(z|X), sample epsilon = N(0,I)\n",
    "# z = z_mean + sqrt(var) * epsilon\n",
    "\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "# inputno = 0/0\n",
    "# inp11 = 0\n",
    "# out11 = 0\n",
    "\n",
    "# def getnewloss(x2):\n",
    "\n",
    "#     outputno = outputno + 1\n",
    "#     return binary_crossentropy(x1,x2)\n",
    "\n",
    "\n",
    "# def getloss(x1):\n",
    "#     # output = outputs[inputno]\n",
    "#     # inputindex = imindex[inputno]\n",
    "#     outputno = 0\n",
    "#     losses1 = tf.map_fn(lambda x : getnewloss(x),outputs)\n",
    "#     inputno = inputno + 1\n",
    "#     return tf.reduce_sum(losses1)\n",
    "\n",
    "\n",
    "\n",
    "def classcrossentropy(inputs,outputs):\n",
    "    loss = 0\n",
    "    i = 0\n",
    "\n",
    "    input1 = inputs[1]\n",
    "    output1 = outputs[0]\n",
    "    recon_loss = -tf.reduce_sum(\n",
    "    input1 * tf.log(1e-5+output1) + \n",
    "    (1-input1) * tf.log(1e-5+1-output1), \n",
    "    axis=1\n",
    ")\n",
    "    input2 = inputs[2]\n",
    "    output2 = outputs[1]\n",
    "    \n",
    "#     print(input2.shape)\n",
    "#     print(output2.shape)\n",
    "    \n",
    "#     print(inputs.shape)\n",
    "    recon_loss = recon_loss + classifierwt*categorical_crossentropy(input2,output2)\n",
    "    \n",
    "    \n",
    "    return recon_loss\n",
    "\n",
    "\n",
    "\n",
    "    # print(tf.shape(inputs))\n",
    "    # print(outputs.shape)\n",
    "    # print(inputs[0].shape)\n",
    "    # print(inputs[1].shape)\n",
    "    \n",
    "    # print(inputs.shape)\n",
    "    #\n",
    "    # return binary_crossentropy(inputs[1],outputs)\n",
    "    \n",
    "#     tf.map_fn(\n",
    "#     fn,\n",
    "#     elems,\n",
    "#     dtype=None,\n",
    "#     parallel_iterations=None,\n",
    "#     back_prop=True,\n",
    "#     swap_memory=False,\n",
    "#     infer_shape=True,\n",
    "#     name=None\n",
    "# )\n",
    "                                   \n",
    "    # inp11 = inputs[0]\n",
    "    # out11 = outputs                                                     \n",
    "    # iminput = inputs[0]\n",
    "    # imindex = inputs[1]\n",
    "    # indices = [i for i, x in enumerate(my_list) if x == \"whatever\"]\n",
    "    # outputs[:] = outputs[:].append(imindex)\n",
    "    # for rec in outputs:\n",
    "        # loss = rec\n",
    "    # outputno = 0\n",
    "    # inputno = 0\n",
    "    # losses = tf.map_fn(lambda x : getloss(x),)\n",
    "\n",
    "    # return tf.reduce_sum(losses,axis = 1)\n",
    "\n",
    "    # inputs = inputs[0]\n",
    "\n",
    "    # epsilon = 1e-10\n",
    "    # epsilon = epsilon*np.ones((128,2048))\n",
    "\n",
    "    # recon_loss = -tf.reduce_sum(\n",
    "    #     inputs * tf.log(1e-10+outputs) + \n",
    "    #     (1-inputs) * tf.log(1e-10+1-outputs), \n",
    "    #     axis=1\n",
    "    # )\n",
    "\n",
    "    # recon_loss = tf.reduce_mean(recon_loss)\n",
    "\n",
    "    # Latent loss\n",
    "    # KL divergence: measure the difference between two distributions\n",
    "    # Here we measure the divergence between \n",
    "    # the latent distribution and N(0, 1)\n",
    "    # latent_loss = -0.5 * tf.reduce_sum(\n",
    "    #     1 + z_log_sigma_sq - tf.square(z_mu) - \n",
    "    #     tf.exp(z_log_sigma_sq), axis=1)\n",
    "    # latent_loss = tf.reduce_mean(latent_loss)\n",
    "\n",
    "    # total_loss = recon_loss + latent_loss\n",
    "    # return recon_loss\n",
    "\n",
    "    # j = 0\n",
    "    # # return binary_crossentropy(inputs[0],outputs)\n",
    "    # inputim = inputs[0]\n",
    "    # inputlabel = inputs[1]\n",
    "\n",
    "    # for i in range(batch_size):\n",
    "    #     # j = 0\n",
    "    #     for j in range(batch_size):\n",
    "    #         # j+= 1\n",
    "    #         # input1 = inputim[i]\n",
    "    #         input2 = inputim[j]\n",
    "    #         k1 = inputlabel[i]\n",
    "    #         k2 = inputlabel[j]\n",
    "    #         output1 = outputs[i]\n",
    "    #         # print(k1)\n",
    "    #         # print(k2)\n",
    "    #         # if(k1 == k2):\n",
    "    #         loss = loss + binary_crossentropy(input2,output1)\n",
    "    #     # i+= 1\n",
    "    # return loss\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # print(inputs1.shape)\n",
    "    # # if(runmode == \"single\"):\n",
    "    # # return binary_crossentropy(inputs,outputs)\n",
    "    \n",
    "\n",
    "    # i = 0\n",
    "    # for inputs in inputs1:\n",
    "    #     outputs = outputs1[i]\n",
    "    #     imlabel = 0\n",
    "    #     i1 = 0\n",
    "    #     for im1 in flowerlist:\n",
    "    #         # if(im1 == inputs):\n",
    "    #         if(np.array_equal(im1,inputs)):\n",
    "    #             imlabel = labels[i1]\n",
    "    #             break\n",
    "    #         i1 = i1+1\n",
    "    #     print(imlabel)\n",
    "    # #     for key in dict1:    \n",
    "    #     loss = 0\n",
    "    #     i1 = 0\n",
    "    #     for image1 in flowerlist:\n",
    "    #         if(labels[i1] == imlabel):\n",
    "    #             if(loss == 0):\n",
    "    #                 loss = binary_crossentropy(image1,outputs)\n",
    "    #             else:\n",
    "    #                 loss = loss + binary_crossentropy(image1,outputs)\n",
    "    #         i1 += 1\n",
    "    #     # return binary_crossentropy(inputs,outputs)\n",
    "    #     print(loss)\n",
    "    #     print(binary_crossentropy(image1,outputs))\n",
    "    #     i = i+1\n",
    "    # return K.sum(loss)\n",
    "\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "def sampling(args):\n",
    "    \"\"\"Reparameterization trick by sampling from an isotropic unit Gaussian.\n",
    "\n",
    "    # Arguments\n",
    "        args (tensor): mean and log of variance of Q(z|X)\n",
    "\n",
    "    # Returns\n",
    "        z (tensor): sampled latent vector\n",
    "    \"\"\"\n",
    "\n",
    "    z_mean, z_log_var = args\n",
    "    batch = K.shape(z_mean)[0]\n",
    "    dim = K.int_shape(z_mean)[1]\n",
    "    # by default, random_normal has mean = 0 and std = 1.0\n",
    "    epsilon = K.random_normal(shape=(batch, dim))\n",
    "    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "\n",
    "def plot_results(models,\n",
    "                 data,\n",
    "                 batch_size=128,\n",
    "                 model_name=\"vae_mnist\"):\n",
    "    \"\"\"Plots labels and MNIST digits as a function of the 2D latent vector\n",
    "\n",
    "    # Arguments\n",
    "        models (tuple): encoder and decoder models\n",
    "        data (tuple): test data and label\n",
    "        batch_size (int): prediction batch size\n",
    "        model_name (string): which model is using this function\n",
    "    \"\"\"\n",
    "\n",
    "    encoder, decoder = models\n",
    "    x_test, y_test = data\n",
    "    os.makedirs(model_name, exist_ok=True)\n",
    "\n",
    "    filename = os.path.join(model_name, \"vae_mean.png\")\n",
    "    # display a 2D plot of the digit classes in the latent space\n",
    "    z_mean, _, _ = encoder.predict(x_test,\n",
    "                                   batch_size=batch_size)\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    plt.scatter(z_mean[:, 0], z_mean[:, 1], c=y_test)\n",
    "    plt.colorbar()\n",
    "    plt.xlabel(\"z[0]\")\n",
    "    plt.ylabel(\"z[1]\")\n",
    "    plt.savefig(filename)\n",
    "    plt.show()\n",
    "\n",
    "    filename = os.path.join(model_name, \"digits_over_latent.png\")\n",
    "    # display a 30x30 2D manifold of digits\n",
    "    n = 30\n",
    "    digit_size = 28\n",
    "    figure = np.zeros((digit_size * n, digit_size * n))\n",
    "    # linearly spaced coordinates corresponding to the 2D plot\n",
    "    # of digit classes in the latent space\n",
    "    grid_x = np.linspace(-4, 4, n)\n",
    "    grid_y = np.linspace(-4, 4, n)[::-1]\n",
    "\n",
    "    for i, yi in enumerate(grid_y):\n",
    "        for j, xi in enumerate(grid_x):\n",
    "            z_sample = np.array([[xi, yi]])\n",
    "            x_decoded = decoder.predict(z_sample)\n",
    "            digit = x_decoded[0].reshape(digit_size, digit_size)\n",
    "            figure[i * digit_size: (i + 1) * digit_size,\n",
    "                   j * digit_size: (j + 1) * digit_size] = digit\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    start_range = digit_size // 2\n",
    "    end_range = (n - 1) * digit_size + start_range + 1\n",
    "    pixel_range = np.arange(start_range, end_range, digit_size)\n",
    "    sample_range_x = np.round(grid_x, 1)\n",
    "    sample_range_y = np.round(grid_y, 1)\n",
    "    plt.xticks(pixel_range, sample_range_x)\n",
    "    plt.yticks(pixel_range, sample_range_y)\n",
    "    plt.xlabel(\"z[0]\")\n",
    "    plt.ylabel(\"z[1]\")\n",
    "    plt.imshow(figure, cmap='Greys_r')\n",
    "    plt.savefig(filename)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# In[6]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[4]:\n",
    "# folder = \"jpg\"\n",
    "def load_csv(filename):\n",
    "    dataset = list()\n",
    "    with open(filename, 'r') as file:\n",
    "        csv_reader = reader(file)\n",
    "        for row in csv_reader:\n",
    "            if not row:\n",
    "                continue\n",
    "            dataset.append(row)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "# In[9]:\n",
    "\n",
    "\n",
    "# In[17]:\n",
    "\n",
    "\n",
    "# dict1 = {}\n",
    "# mypath = \"/home/anurag/projects/VAEProj/102flowers/jpg\"\n",
    "# onlyfiles = [f for f in listdir(mypath) if isfile(join(mypath, f))]\n",
    "# sort(onlyfiles)\n",
    "# onlyfiles.sort()\n",
    "filename = './flowersXception.csv'\n",
    "\n",
    "featurestotal = load_csv(filename)\n",
    "# labels = labels[0]    \n",
    "# print(onlyfiles)\n",
    "# i1 = 1\n",
    "# seltol = 1300\n",
    "#seltol = 10\n",
    "# selected1 = 800\n",
    "# for selected1 in [200]:\n",
    "# samplpositive = 600\n",
    "#samplpositive = 10\n",
    "# featurestotal = []\n",
    "# i1 = 0\n",
    "# for image in onlyfiles:\n",
    "#     if(image[-3:] != \"jpg\"):\n",
    "#         continue\n",
    "#     # i1 = i1 + 1\n",
    "#     # print(\"efrffrwf\")\n",
    "\n",
    "#     im1 = mypath+\"/\"+ image\n",
    "#     # print(im1)\n",
    "#     # print(labels[i1])\n",
    "#     # im = imread(im1,cv2.COLOR_BGR2GRAY)\n",
    "#     im = imread(im1)\n",
    "#     im = im/float(255)\n",
    "#     # print(im)\n",
    "#     res = cv2.resize(im, dsize=(50,50), interpolation=cv2.INTER_CUBIC)\n",
    "#     #res = cv2.resize(im, dsize=(500,500))\n",
    "#     # res = im\n",
    "#     # k1\n",
    "\n",
    "\n",
    "#     k1 = np.reshape(res,-1)\n",
    "#     # np.append(k1,labels[i1])\n",
    "#     # print(k)\n",
    "#     k1 = list(k1)\n",
    "#     k1.append(labels[i1])\n",
    "#     # print(k1)\n",
    "\n",
    "# #     dict1[k1] = labels[i1] \n",
    "#     # print(k1.shape)\n",
    "#         # print(i1)\n",
    "#     print(i1)\n",
    "#     featurestotal.append(k1)\n",
    "#     i1 = i1+ 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     # if(i1 == 85):\n",
    "#         # break\n",
    "\n",
    "from PIL import Image\n",
    "k22 = 0\n",
    "for image11 in featurestotal:\n",
    "    img11 = (((np.array(image11[:2048])).reshape(32,64)).astype(np.float)*255).astype(np.uint8)\n",
    "#     x = \"255\"\n",
    "#     img11 = np.dot(img11,255)\n",
    "#     img11 = list(img11)\n",
    "#     print(img11)\n",
    "#     array = (np.random.rand(100, 200)*256).astype(np.uint8)\n",
    " \n",
    "    img = Image.fromarray(img11)\n",
    "#     img.save('test.png')\n",
    "    \n",
    "#     img22 = img11*255\n",
    "#     img = Image.fromarray(img11*255)\n",
    "    img.show()\n",
    "    \n",
    "    k22+=1\n",
    "    if(k22 == 20):\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.69266358e-01 0.00000000e+00 3.78760219e-01 ... 1.48642763e-01\n",
      "  2.81311899e-01 5.90000000e+01]\n",
      " [1.79004803e-01 0.00000000e+00 8.66748393e-02 ... 2.86119431e-01\n",
      "  2.34280173e-02 5.80000000e+01]\n",
      " [1.01077884e-01 0.00000000e+00 3.21545631e-01 ... 4.29836661e-02\n",
      "  1.25668785e-02 8.00000000e+00]\n",
      " ...\n",
      " [5.76116750e-03 0.00000000e+00 5.39481342e-01 ... 7.25073814e-02\n",
      "  2.64556855e-01 7.20000000e+01]\n",
      " [0.00000000e+00 5.63756889e-03 3.36055845e-01 ... 2.73877174e-01\n",
      "  1.21015813e-02 4.30000000e+01]\n",
      " [0.00000000e+00 4.36663907e-03 5.11332136e-03 ... 0.00000000e+00\n",
      "  1.30717759e-04 3.20000000e+01]]\n",
      "[[2.73540139e-01 8.16171756e-04 3.16243976e-01 ... 2.83945024e-01\n",
      "  8.08510277e-03 6.50000000e+01]\n",
      " [1.47516400e-01 1.21126799e-02 4.36980538e-02 ... 7.45755956e-02\n",
      "  2.95756370e-01 7.10000000e+01]\n",
      " [1.00970037e-01 0.00000000e+00 4.45184708e-01 ... 1.06717512e-01\n",
      "  2.73416489e-02 5.80000000e+01]\n",
      " ...\n",
      " [1.44163554e-03 4.59111072e-02 2.39384875e-01 ... 8.33705650e-04\n",
      "  5.33285975e-01 8.00000000e+01]\n",
      " [0.00000000e+00 0.00000000e+00 1.08605392e-01 ... 7.11154286e-03\n",
      "  2.18522546e-04 5.10000000e+01]\n",
      " [2.52680238e-02 2.68462487e-03 6.51264489e-02 ... 6.17234588e-01\n",
      "  1.66951165e-01 9.40000000e+01]]\n",
      "[65. 71. 58. 73. 44. 27. 80. 51. 94.]\n",
      "[59. 58.  8. ... 72. 43. 32.]\n"
     ]
    }
   ],
   "source": [
    "# flowerlist = []\n",
    "\n",
    "rd.shuffle(featurestotal)\n",
    "# flowerlist.extend(featurestotal)\n",
    "\n",
    "# flowerlist contains the required entries\n",
    "\n",
    "\n",
    "\n",
    "x_train = featurestotal[:8180]\n",
    "x_test = featurestotal[8180:]\n",
    "\n",
    "\n",
    "# x_train = featurestotal[:4000]\n",
    "# x_test = featurestotal[4000:]\n",
    "\n",
    "\n",
    "# x_train = featurestotal[:400]\n",
    "# x_test = featurestotal[400:]\n",
    "\n",
    "\n",
    "x_train = np.array(x_train,dtype = float)\n",
    "x_test = np.array(x_test,dtype = float)\n",
    "\n",
    "print(x_train)\n",
    "print(x_test)\n",
    "\n",
    "\n",
    "x_trainlabel = x_train[:,2048]\n",
    "x_train = x_train[:,0:2048]\n",
    "x_testlabel = x_test[:,2048]\n",
    "x_test = x_test[:,0:2048]\n",
    "\n",
    "print(x_testlabel)\n",
    "print(x_trainlabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.zeros(2048,dtype = float)\n",
    " # = np.repeat(a[:, :, np.newaxis], 3, axis=2)\n",
    "\n",
    "x_trainlabelsum = np.repeat(a[np.newaxis,:], 102,axis = 0)\n",
    "x_testlabelsum = np.repeat(a[np.newaxis,:], 102,axis = 0)\n",
    "print(x_trainlabelsum[0].shape)\n",
    "\n",
    "# x_testlabelsum = []\n",
    "\n",
    "\n",
    "# In[18]:\n",
    "\n",
    "\n",
    "trainnum = np.zeros(102)\n",
    "testnum = np.zeros(102)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# In[18]:\n",
    "trainonehotlabel = []\n",
    "testonehotlabel = []\n",
    "\n",
    "\n",
    "i = 0\n",
    "for fq in x_trainlabel:\n",
    "    image = x_train[i]\n",
    "    fq = int(fq)\n",
    "    # print(fq)\n",
    "    fq = fq-1\n",
    "#     print(fq)\n",
    "#     print(x_trainlabelsum[fq].shape)\n",
    "#     print(type(image))\n",
    "#     print(image.shape)\n",
    "#     print(x_trainlabelsum[fq])\n",
    "# #     print(type(image))\n",
    "#     print(image)\n",
    "    \n",
    "    x_trainlabelsum[fq] = np.add(x_trainlabelsum[fq],image)\n",
    "    trainnum[fq] = trainnum[fq] + 1\n",
    "    arr = np.zeros(102)\n",
    "    arr[fq] = 1\n",
    "    trainonehotlabel.append(arr)\n",
    "\n",
    "    i = i+1\n",
    "\n",
    "i = 0\n",
    "for fq in x_testlabel:\n",
    "    fq = fq-1\n",
    "    im = x_test[i]\n",
    "    fq = int(fq)\n",
    "    x_testlabelsum[fq] = np.add(x_testlabelsum[fq],im)\n",
    "    testnum[fq] = testnum[fq] + 1\n",
    "\n",
    "    arr = np.zeros(102)\n",
    "    arr[fq] = 1\n",
    "    testonehotlabel.append(arr)\n",
    "\n",
    "    i = i+1\n",
    "\n",
    "print(x_trainlabel[0:4])\n",
    "print(x_testlabel)\n",
    "\n",
    "\n",
    "trainonehotlabel = np.array(trainonehotlabel)\n",
    "testonehotlabel = np.array(testonehotlabel)\n",
    "print(trainonehotlabel)\n",
    "print(testonehotlabel)\n",
    "\n",
    "# exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train2 = np.copy(x_train)\n",
    "x_test2 = np.copy(x_test)\n",
    "\n",
    "i = 0\n",
    "for d2 in x_trainlabel:\n",
    "    d2 = int(d2)\n",
    "    d2 = d2-1\n",
    "    x_train2[i] = x_trainlabelsum[d2]/(float(trainnum[d2]))\n",
    "    i = i+1\n",
    "\n",
    "i = 0\n",
    "for d2 in x_testlabel:\n",
    "    d2 = int(d2)\n",
    "    d2 = d2-1\n",
    "    x_test2[i] = x_testlabelsum[d2]/(float(trainnum[d2]))\n",
    "    i = i+1\n",
    "\n",
    "# x_train\n",
    "\n",
    "# x_trainlabel = np.array(x_trainlabel)\n",
    "# x_testlabel = np.array(x_testlabel)\n",
    "\n",
    "# x_train\n",
    "\n",
    "\n",
    "# print(classcrossentropy(x_train[0],x_train[1]))\n",
    "# MNIST dataset\n",
    "# (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "\n",
    "# In[19]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_size = x_train[0].shape[0]\n",
    "# print(image_size)\n",
    "original_dim = 2048\n",
    "# x_train = np.reshape(x_train, [-1, original_dim])\n",
    "# x_test = np.reshape(x_test, [-1, original_dim])\n",
    "# x_train = x_train.astype('float32') / 255\n",
    "# x_test = x_test.astype('float32') / 255\n",
    "# print(im)\n",
    "# network parameters\n",
    "input_shape = (original_dim, )\n",
    "# print(\"input_shape\")\n",
    "# print(input_shape)\n",
    "intermediate_dim = 512 # of original layers\n",
    "intermediate_dim1 = 512 # of extra layers\n",
    "classifierdim = 512 # of classifiers\n",
    "classifieroutputdim = 102\n",
    "\n",
    "batch_size = 128\n",
    "latent_dim = 2\n",
    "epochs = 300\n",
    "\n",
    "\n",
    "# model.add(Dense(60, input_dim=60, kernel_initializer='normal', activation='relu', kernel_constraint=maxnorm(3)))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(Dense(30, kernel_initializer='normal', activation='relu', kernel_constraint=maxnorm(3)))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
    "\n",
    "# VAE model = encoder + decoder\n",
    "# build encoder model\n",
    "input1 = Input(shape=input_shape, name='encoder_input_im')\n",
    "input2 = Input(shape=input_shape, name='encoder_input_imsum')\n",
    "input3 = Input(shape=(classifieroutputdim,),name='encoder_input_labels')\n",
    "# inputs = Input(shape=1, name='encoder_input_label')\n",
    "# drop1 = Dropout(0.2)(input1)\n",
    "x = Dense(intermediate_dim, activation='relu')(input1)\n",
    "x = Dense(intermediate_dim1, activation='relu')(x)\n",
    "# x = Dense(intermediate_dim1, activation='relu')(x)\n",
    "\n",
    "# x = Dense(intermediate_dim1, activation='relu')(x)\n",
    "# drop1 = Dropout(0.2)(x)\n",
    "z_mean = Dense(latent_dim, name='z_mean')(x)\n",
    "z_log_var = Dense(latent_dim, name='z_log_var')(x)\n",
    "\n",
    "inps = [input1,input2,input3]\n",
    "# use reparameterization trick to push the sampling out as input\n",
    "# note that \"output_shape\" isn't necessary with the TensorFlow backend\n",
    "z = Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])\n",
    "\n",
    "# instantiate encoder model\n",
    "\n",
    "# encoder = Model(inputs, [z_mean, z_log_var, z], name='encoder')\n",
    "# encoder = Model(inputs = [input1,input2], [z_mean, z_log_var, z], name='encoder')\n",
    "encoder = Model(inps, [z_mean, z_log_var, z], name='encoder')\n",
    "encoder.summary()\n",
    "# plot_model(encoder, to_file='vae_mlp_encoder.png', show_shapes=True)\n",
    "\n",
    "# build decoder model\n",
    "latent_inputs = Input(shape=(latent_dim,), name='z_samples')\n",
    "\n",
    "x = Dense(intermediate_dim, activation='relu')(latent_inputs)\n",
    "# drop2 = Dropout(0.2)(x)\n",
    "# x = Dense(intermediate_dim1, activation='relu')(x)\n",
    "x = Dense(intermediate_dim1, activation='relu')(x)\n",
    "# x = Dense(intermediate_dim1, activation='relu')(x)\n",
    "\n",
    "outputs = Dense(original_dim, activation='sigmoid')(x)\n",
    "\n",
    "\n",
    "# instantiate decoder model\n",
    "decoder = Model(latent_inputs, outputs, name='decoder')\n",
    "decoder.summary()\n",
    "# plot_model(decoder, to_file='vae_mlp_decoder.png', show_shapes=True)\n",
    "\n",
    "#building classifier models\n",
    "classifier_inputs = Input(shape=(latent_dim,), name=\"class1\")\n",
    "\n",
    "x = Dense(classifierdim,activation = 'relu')(classifier_inputs)\n",
    "# x = Dense(classifierdim,activation = 'relu')(x)  # additional layer\n",
    "classifier_output = Dense(classifieroutputdim,activation = 'softmax')(x)\n",
    "\n",
    "classifier = Model(classifier_inputs,classifier_output,name=\"classifier\")\n",
    "\n",
    "classifier.summary()\n",
    "\n",
    "# instantiate VAE model\n",
    "output1 = decoder(encoder(inps)[2])\n",
    "output2 = classifier(encoder(inps)[2]) # classifing on smaple values\n",
    "# output2 = classifier(encoder(inps)[0]) # classifing on z_means\n",
    "outputs = [output1,output2]\n",
    "\n",
    "vae = Model(inps, outputs, name='vae_mlp + classifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs = Input(shape=input_shape, name='encoder_input')\n",
    "# x = Dense(intermediate_dim, activation='relu')(inputs)\n",
    "# z_mean = Dense(latent_dim, name='z_mean')(x)\n",
    "# z_log_var = Dense(latent_dim, name='z_log_var')(x)\n",
    "\n",
    "# # use reparameterization trick to push the sampling out as input\n",
    "# # note that \"output_shape\" isn't necessary with the TensorFlow backend\n",
    "# z = Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])\n",
    "\n",
    "# # instantiate encoder model\n",
    "# encoder = Model(inputs, [z_mean, z_log_var, z], name='encoder')\n",
    "# encoder.summary()\n",
    "# # plot_model(encoder, to_file='vae_mlp_encoder.png', show_shapes=True)\n",
    "\n",
    "# # build decoder model\n",
    "# latent_inputs = Input(shape=(latent_dim,), name='z_sampling')\n",
    "# x = Dense(intermediate_dim, activation='relu')(latent_inputs)\n",
    "# outputs = Dense(original_dim, activation='sigmoid')(x)\n",
    "\n",
    "# # instantiate decoder model\n",
    "# decoder = Model(latent_inputs, outputs, name='decoder')\n",
    "# decoder.summary()\n",
    "# # plot_model(decoder, to_file='vae_mlp_decoder.png', show_shapes=True)\n",
    "\n",
    "# # instantiate VAE model\n",
    "# outputs = decoder(encoder(inputs)[2])\n",
    "# vae = Model(inputs, outputs, name='vae_mlp')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # dict1 = {}\n",
    "    kllosswt = 1\n",
    "    classifierwt = 0.5\n",
    "#     parser = argparse.ArgumentParser()\n",
    "\n",
    "#     help_ = \"Load h5 model trained weights\"\n",
    "#     parser.add_argument(\"-w\", \"--weights\", help=help_)\n",
    "#     help_ = \"Use mse loss instead of binary cross entropy (default)\"\n",
    "#     parser.add_argument(\"-m\",\n",
    "#                         \"--mse\",\n",
    "#                         help=help_, action='store_true')\n",
    "#     args = parser.parse_args()\n",
    "    models = (encoder, decoder)\n",
    "    # data = (x_test, y_test)\n",
    "\n",
    "# here1\n",
    "    # VAE loss = mse_loss or xent_loss + kl_loss\n",
    "    \n",
    "    # runmode = input(\"single or class encoder \")\n",
    "    # z33 = np.identity(1000)\n",
    "\n",
    "\n",
    "#     if args.mse:\n",
    "#         reconstruction_loss = mse(inputs, outputs)\n",
    "#     else:\n",
    "    reconstruction_loss = classcrossentropy(inps,\n",
    "                                              outputs)\n",
    "\n",
    "    reconstruction_loss *= original_dim\n",
    "    kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n",
    "    kl_loss = K.sum(kl_loss, axis=-1)\n",
    "    kl_loss = kl_loss*-0.5*kllosswt\n",
    "    vae_loss = K.mean(reconstruction_loss + kl_loss)\n",
    "    vae.add_loss(vae_loss)\n",
    "    vae.compile(optimizer='adam')\n",
    "    vae.summary()\n",
    "    # plot_model(vae,\n",
    "               # to_file='vae_mlp.png',\n",
    "               # show_shapes=True)\n",
    "\n",
    "#     if args.weights:\n",
    "#         vae.load_weights(args.weights)\n",
    "#     else:\n",
    "        # train the autoencoder\n",
    "    vae.fit([x_train,x_train2,trainonehotlabel],\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            validation_data=([x_test,x_test2,testonehotlabel], None))\n",
    "    vae.save_weights('vae_mlp_mnist.h5')\n",
    "\n",
    "\n",
    "\n",
    "    # for i in test2\n",
    "    # plot_results(models,\n",
    "                 # data,\n",
    "                 # batch_size=batch_size,\n",
    "                 # model_name=\"vae_mlp\")\n",
    "\n",
    "    encoder, decoder = models\n",
    "    # encoder, decoder = models\n",
    "    x_test, y_test = [x_train,x_train2,trainonehotlabel],x_trainlabel\n",
    "    # os.makedirs(model_name, exist_ok=True)\n",
    "\n",
    "    # filename = os.path.join(model_name, \"vae_mean.png\")\n",
    "    # display a 2D plot of the digit classes in the latent space\n",
    "    z_mean, _, _ = encoder.predict(x_test,\n",
    "                                   batch_size=batch_size)\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    plt.scatter(z_mean[:, 0], z_mean[:, 1], c=y_test)\n",
    "    plt.colorbar()\n",
    "    plt.xlabel(\"z[0]\")\n",
    "    plt.ylabel(\"z[1]\")\n",
    "    # plt.savefig(filename)\n",
    "    plt.show()\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "# In[ ]:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
